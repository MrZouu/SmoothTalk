import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms, models, datasets
import os

# Vérifier si un GPU est disponible
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
print("Using the ASL DATASET")

# Définir la transformation des données
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalisation des valeurs de pixel
])

# Définir le chemin vers le dossier contenant le dataset ASL Alphabet
data_path = 'C:\\Users\\thoma\\OneDrive\\Bureau\\COURS\\ING4\\PPE\\SmoothTalk\\data\\ASL'

# Charger le jeu de données ASL Alphabet
asl_dataset = datasets.ImageFolder(root=data_path, transform=transform)

# Diviser le jeu de données en ensembles d'entraînement et de test
dataset_size = len(asl_dataset)
train_size = int(0.8 * dataset_size)
test_size = dataset_size - train_size
train_dataset, test_dataset = random_split(asl_dataset, [train_size, test_size])

# Définir les chargeurs de données
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Remplacer le modèle CNN actuel par ResNet50
class ResNet50(nn.Module):
    def __init__(self):
        super(ResNet50, self).__init__()
        # Charger le modèle pré-entraîné ResNet50
        resnet = models.resnet50(pretrained=True) #possible change cause pretrained is deprecated : resnet = models.resnet50(weights='imagenet')
        
        # Extraire les couches sauf la couche de classification finale (dernière couche)
        self.features = nn.Sequential(*list(resnet.children())[:-1])

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        return x

# Instancier le modèle ResNet50, la fonction de coût et l'optimiseur
model = ResNet50().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Entraîner le modèle
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    for batch_idx, (inputs, labels) in enumerate(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        if batch_idx % 100 == 0:
            print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}')

    # Tester le modèle sur les données de test après chaque époque
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_idx, (inputs, labels) in enumerate(test_loader):
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            if batch_idx % 100 == 0:
                print(f'Testing, Batch {batch_idx}/{len(test_loader)}, Accuracy: {(correct / total) * 100:.2f}%')

    accuracy = correct / total
    print(f'Accuracy on test data after epoch {epoch+1}: {accuracy * 100:.2f}%')
